# âœˆï¸ A320 Maintenance RAG Chatbot

A Retrieval-Augmented Generation (RAG) based AI chatbot built using **Django**, **Pinecone**, **HuggingFace Embeddings**, and **Groq LLM**.  

This system allows users to query a technical PDF document (A320 Aircraft Characteristics â€“ Airport & Maintenance Planning) and receive context-grounded answers.

---

## ğŸš€ Features

- ğŸ“„ PDF-based knowledge system
- ğŸ” Semantic search using vector embeddings
- ğŸ§  Context-aware AI responses (RAG architecture)
- âš¡ Fast LLM inference using Groq (Llama 3.1)
- ğŸ¨ Modern animated UI (Glassmorphism design)
- ğŸ” Secure API key management via `.env`

---

## ğŸ—ï¸ Architecture
```
Frontend (HTML/JS)                         User Question
        â†“                                        â†“
Django API (/ask)                          Django Backend
        â†“                                        â†“
Embed Query (BGE)                    SentenceTransformer Embedding
        â†“                                        â†“
Pinecone (Top-k chunks)                  Pinecone Vector Search
        â†“                                        â†“
Construct Prompt                          Retrieve Top Chunk
        â†“                                        â†“
Gemini API                               Groq LLM (Llama 3.1)
        â†“                                        â†“
Return Answer                           Context-Grounded Answer
```


---

## ğŸ§  How RAG Works in This Project

1. PDF is parsed and chunked (in Google Colab).
2. Each chunk is converted into embeddings.
3. Embeddings are stored in Pinecone.
4. When user asks a question:
   - Question is embedded.
   - Most relevant chunk is retrieved.
   - Retrieved context is sent to Groq LLM.
   - LLM generates answer strictly from context.

---

## ğŸ› ï¸ Tech Stack

| Layer        | Technology Used |
|--------------|-----------------|
| Backend      | Django          |
| Embeddings   | BAAI/bge-small-en-v1.5 |
| Vector DB    | Pinecone (Free Tier) |
| LLM          | Groq (Llama 3.1-8b-instant) |
| Frontend     | HTML + CSS + JS |
| Deployment   | Render (Planned) |

---

## ğŸ“¦ Installation

### 1ï¸âƒ£ Clone Repository

```bash
git clone <your-repo-url>
cd rag_backend
```
### 2ï¸âƒ£ Create Virtual Environment

```
python -m venv venv
source venv/bin/activate  # Mac/Linux
venv\Scripts\activate     # Windows
```

### 3ï¸âƒ£ Install Dependencies
```
pip install -r requirements.txt
```
If requirements.txt not created, install manually:
```
pip install django
pip install sentence-transformers
pip install pinecone-client
pip install groq
pip install python-dotenv
```
### ğŸ” Environment Variables

Create a .env file in project root:

```
PINECONE_API_KEY=your_pinecone_key
GROQ_API_KEY=your_groq_key
```
Add .env to .gitignore:
```
.env
venv/
__pycache__/
```
### â–¶ï¸ Running the Project
```
python manage.py migrate
python manage.py runserver
```
Open browser:
```
http://127.0.0.1:8000/
```
ğŸ“‚ Project Structure
```
rag_backend/
â”‚
â”œâ”€â”€ manage.py
â”œâ”€â”€ .env
â”‚
â”œâ”€â”€ rag_backend/
â”‚   â”œâ”€â”€ settings.py
â”‚   â”œâ”€â”€ urls.py
â”‚
â””â”€â”€ chatbot/
    â”œâ”€â”€ views.py
    â”œâ”€â”€ rag_utils.py
    â”œâ”€â”€ urls.py
    â”œâ”€â”€ templates/
    â”‚     â””â”€â”€ index.html
    â””â”€â”€ static/
          â””â”€â”€ style.css
```
### ğŸ§ª Sample Test Questions

- What is the maximum ramp weight of the A320-200?
- Describe the jacking procedure for maintenance.
- What wind limits apply during aircraft jacking?
- What protections are provided by the engine nacelle?

### Out-of-scope questions should return:

"Not available in document."

### âš™ï¸ Configuration Details
Embedding Model
```
BAAI/bge-small-en-v1.5
Dimension: 384
Similarity: Cosine
```
LLM Model
```
llama-3.1-8b-instant
Retrieval Strategy
```
Top K = 1

Context limited to 1200 characters

Max output tokens = 300
